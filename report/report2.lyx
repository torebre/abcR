#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
% Først spesifiserer vi hvilken dokumentklasse vi vil ha og noen 
% globale opsjoner. Bytt ut 'article' med 'book' hvis du vil ha 
% med kapitler.
%\documentclass[a4paper, twoside, titlepage, 11pt]{article}

% Så sier vi fra om hvilke tilleggspakker vi trenger
% til dokumentet vårt. De som du ikke trenger (se kommentaren) 
% kan det være en fordel å kommentere ut (sett prosenttegn foran),
% da vil kompilering gå raskere.

%\usepackage[norsk]{babel}	% norske navn rundt omkring
\usepackage[T1]{fontenc}		% norsk tegnsett (æøå)
%\usepackage[utf8]{inputenc}	% norsk tegnsett
\usepackage{geometry}		% anbefalt pakke for å styre marger.

\usepackage{amsmath,amsfonts,amssymb} % matematikksymboler
\usepackage{amsthm}                   % for å lage teoremer og lignende.
\usepackage{graphicx}                 % inkludering av grafikk
\usepackage{subfig}                   % hvis du vil kunne ha flere
                                      % figurer inni en figur
\usepackage{listings}                 % Fin for inkludering av kildekode

%\usepackage{hyperref}                % Lager hyperlinker i evt. pdf-dokument
                                      % men har noen bugs, så den er kommentert
                                      % bort her.
                                 
% Indeksgenerering er kommentert ut her. Ta bort prosenttegnene
% hvis du vil ha en indeks:
%\usepackage{makeidx}     
%\makeindex              


\usepackage{algpseudocode}


% \begin{document}

\pagestyle{empty}
\pagenumbering{roman}

% Inkluder forsida:
%\input{forside}

% Romerske tall på alt før selve rapporten starter er pent.
\pagenumbering{roman}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8-plain
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand include
filename "forside.tex"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{arabic} 
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Quickly about the problem and where ABC fits in.
 Overview of the report
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Bayesian inference
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Short introduction to Bayesian inference
\end_layout

\end_inset


\end_layout

\begin_layout Standard
See 
\begin_inset CommandInset citation
LatexCommand cite
key "gelman2013bayesian"

\end_inset

 for a full treatment on all aspects of Bayesian data analysis.
\end_layout

\begin_layout Standard
In the following 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 denotes a vector of model parameters, and 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 is a vector of observed data.
 Our aim is to make inferences about 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 that are consistent with the observed data.
 The process of data analysis can be viewed as consisting of three steps:
\end_layout

\begin_layout Enumerate
Setting up a full probability model.
 This means formulating a joint probability model for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

: 
\begin_inset Formula $p(\boldsymbol{\theta}|\boldsymbol{x})$
\end_inset

.
\end_layout

\begin_layout Enumerate
Conditioning on the observed data.
 This entails looking at an posterior distribution, explained below.
\end_layout

\begin_layout Enumerate
Evaluating the model.
 If the model does not fit the observed data, it can be changed, and the
 process starts from step 1 again.
\end_layout

\begin_layout Standard
In step 2 above, we need to make inferences about 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

, which are the variables we want to estimate, given the observed data 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

.
 If we just condition on the observed data, we can make use of Bayes' rule
 and write the conditional distribution as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(\boldsymbol{\theta}|\boldsymbol{x})=\frac{p(\boldsymbol{x}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\boldsymbol{x})}
\]

\end_inset


\end_layout

\begin_layout Standard
The factor 
\begin_inset Formula $p(\boldsymbol{x})$
\end_inset

 does not depend on 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

, and since the data is given, that is; 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 does not change, 
\begin_inset Formula $p(\boldsymbol{\theta}|\boldsymbol{x})$
\end_inset

 is a function of 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 only, we can view 
\begin_inset Formula $p(\boldsymbol{x})$
\end_inset

 as a constant.
 We can thus write:
\begin_inset Formula 
\[
p(\boldsymbol{\theta}|\boldsymbol{x})\propto p(\boldsymbol{x}|\boldsymbol{\theta})p(\boldsymbol{\theta})
\]

\end_inset


\end_layout

\begin_layout Standard
There are expressions for the two terms on the right side.
 
\begin_inset Formula $p(\boldsymbol{x}|\boldsymbol{\theta})$
\end_inset

 is termed the likelihood, and noting since 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 is constant, it is a function of 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
 
\begin_inset Formula $p(\boldsymbol{\theta})$
\end_inset

 is termed the prior distribution.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add simple example?
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Approximate Bayesian Computation
\end_layout

\begin_layout Standard
The likelihood function may be difficult to evaluate, for instance it may
 be computationally intractable to do so, or the function does not have
 an analytic form.
 In such settings what are termed likelihood-free methods can be employed
 to draw inferences.
 The name refers to the fact that the methods do not explicitly evaluate
 the likelihood function, but instead approximate it using simulations.
 A requirement is then that we must at least be able to generate samples
 from the likelihood.
\end_layout

\begin_layout Standard
To understand how likelihood-free methods work, let us look at the posterior
 again, and introduce an auxillary variable, 
\begin_inset Formula $\boldsymbol{x}$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Insert reference to likelihood-free Markov chain Monte Carlo
\end_layout

\end_inset

.
 From Bayes rule we have:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(\boldsymbol{\theta},\boldsymbol{x}|\boldsymbol{y})\propto p(\boldsymbol{\theta}|\boldsymbol{x},\boldsymbol{y})p(\boldsymbol{x}|\boldsymbol{\theta})p(\boldsymbol{\theta})
\]

\end_inset


\end_layout

\begin_layout Standard
The posterior can be retrieved by integrating out 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(\boldsymbol{\theta}|\boldsymbol{y})\propto p(\boldsymbol{\theta})\int_{Y}p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\theta})p(\boldsymbol{x}|\boldsymbol{\theta})d\boldsymbol{x}
\]

\end_inset


\end_layout

\begin_layout Standard
It is assumed that we have an expression for the prior, 
\begin_inset Formula $p(\boldsymbol{\theta})$
\end_inset

, and are able to generate samples from 
\begin_inset Formula $p(\boldsymbol{x}|\boldsymbol{\theta}).$
\end_inset

 If 
\begin_inset Formula $p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\theta})$
\end_inset

 is defined to the Dirac-function with 
\begin_inset Formula $\boldsymbol{x}=\boldsymbol{y}$
\end_inset

 we get:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
p(\boldsymbol{\theta}|\boldsymbol{y}) & \propto p(\boldsymbol{\theta})p(\boldsymbol{y}|\boldsymbol{\theta})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
If we were able to draw samples from the likelihood function such that 
\begin_inset Formula $\boldsymbol{x}=\boldsymbol{y}$
\end_inset

, we could estimate the posterior probability.
 A simple algorithm for this is shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "likelihoo-free_alg1"

\end_inset

.
 The algorithm will produce samples from the posterior, but there is a practical
 problem in the comparison 
\begin_inset Formula $\boldsymbol{x}=\boldsymbol{y}$
\end_inset

.
 For continuous probability distributions, the probability that two samples
 are exactly the same is likely to be 0, and for discrete probability distributi
ons the probability for the being the same is also likely to be close to
 0 in practical problems.
 So the algorithm in 
\begin_inset CommandInset ref
LatexCommand ref
reference "likelihoo-free_alg1"

\end_inset

 is not practically useful, since most samples being generated will be discarded.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Likelihood free sampling
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "likelihoo-free_alg1"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}
\end_layout

\begin_layout Plain Layout


\backslash
State $counter 
\backslash
leftarrow 0$
\end_layout

\begin_layout Plain Layout


\backslash
State $samples 
\backslash
leftarrow 
\backslash
text{empty array}$
\end_layout

\begin_layout Plain Layout


\backslash
While{$counter<N$}
\end_layout

\begin_layout Plain Layout


\backslash
State Sample $
\backslash
boldsymbol{
\backslash
theta}$ from $p(
\backslash
boldsymbol{
\backslash
theta})$ 
\end_layout

\begin_layout Plain Layout


\backslash
State Sample $
\backslash
boldsymbol{x}$ from $p(
\backslash
boldsymbol{x}|
\backslash
boldsymbol{
\backslash
theta})$
\end_layout

\begin_layout Plain Layout


\backslash
If{$
\backslash
boldsymbol{x}=
\backslash
boldsymbol{y}$}
\end_layout

\begin_layout Plain Layout


\backslash
State $samples[counter] 
\backslash
leftarrow 
\backslash
boldsymbol{
\backslash
theta}$
\end_layout

\begin_layout Plain Layout


\backslash
State $counter=counter+1$
\end_layout

\begin_layout Plain Layout


\backslash
EndIf
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Keeping the core idea, we can make a couple of changes to make the algorithm
 more practically useful.
 The requirement that 
\begin_inset Formula $\boldsymbol{x}=\boldsymbol{y}$
\end_inset

 can be relaxed, and instead samples that lie within some defined distance
 of 
\begin_inset Formula $\boldsymbol{y}$
\end_inset

 are accepeted as well: 
\begin_inset Formula $d(\boldsymbol{x},\boldsymbol{y})\leq\rho$
\end_inset

.
 This begs the question what 
\begin_inset Formula $\rho$
\end_inset

 should be, and how it affects the approximation.
 Intuitively smaller values yields better approximations, but increases
 computation time, since more samples are discarded.
 We will examine strategies to determine 
\begin_inset Formula $\rho$
\end_inset

 later.
\end_layout

\begin_layout Standard
We also need to make a choice for a distance function 
\begin_inset Formula $d(\boldsymbol{x},\boldsymbol{y})$
\end_inset

.
 If 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{y}$
\end_inset

 are vectors consisting of numbers, the Euclidean distance could for instance
 be used, which may or may not be a good choice.
 In other cases the Euclidean distance measure cannot be used directly on
 the samples, if the sample for instance represents a DNA sequence, which
 does not have a clear numerical representation.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add some examples of what a sample can be (DNA sequence, result from a stochasti
c process)
\end_layout

\end_inset

 Attention also has to be paid to the cost of evaluating 
\begin_inset Formula $d(\boldsymbol{x},\boldsymbol{y})$
\end_inset

.
 The samples can contain large amount of data, again think of a DNA sequence
 as an example.
\end_layout

\begin_layout Standard
If we cannot find a suitable distance measure that works directly on the
 samples, or if it would be computationally intractable to evaluate such
 a distance function, we can instead focus on summary statistics for the
 sample.
 Let 
\begin_inset Formula $\mathrm{S}(\boldsymbol{x})$
\end_inset

 denote the possibly vector valued summary statistic we have chosen for
 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

.
 The main point is that we assume 
\begin_inset Formula $dim(S(\boldsymbol{x}))<dim(\boldsymbol{x})$
\end_inset

 holds, and so it should be computationally cheaper to evaluate 
\begin_inset Formula $d(S(\boldsymbol{x}),S(\boldsymbol{y}))$
\end_inset

.
 Also note that the reduction in dimension in practice renders the probability
 that 
\begin_inset Formula $S(\boldsymbol{x})=S(\boldsymbol{y})$
\end_inset

 greater than that of 
\begin_inset Formula $\boldsymbol{x}=\boldsymbol{y}.$
\end_inset

 This improves the efficiency of algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "likelihoo-free_alg1"

\end_inset

, but again, probably not to such a degree to make it practically useful.
\end_layout

\begin_layout Standard
Ideally we would want 
\begin_inset Formula $\mathrm{S}(\cdot)$
\end_inset

 to be sufficient 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Insert reference to statistical inference book
\end_layout

\end_inset

for the parameters that are being estimated, since then we have the following
 relation, again letting 
\begin_inset Formula $p(S(\boldsymbol{y})|S(\boldsymbol{x}),\boldsymbol{\theta})$
\end_inset

 be the Dirac-delta function:
\begin_inset Formula 
\begin{align*}
p(\boldsymbol{\theta},S(\boldsymbol{x})|S(\boldsymbol{y})) & \propto p(\boldsymbol{\theta})\int_{S(Y)}p(S(\boldsymbol{y})|S(\boldsymbol{x}),\boldsymbol{\theta})p(S(\boldsymbol{x})|\boldsymbol{\theta})dS(\boldsymbol{x})\\
 & =p(\boldsymbol{\theta})p(S(\boldsymbol{y})|\boldsymbol{\theta})\\
 & \text{by suffiency}\\
 & =p(\boldsymbol{\theta})p(\boldsymbol{y}|\boldsymbol{\theta})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add section about distance measures on statistics
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Several strategies have been developed for dealing with the difficulties
 in ABC: Finding suitable statistics automatically and improving the efficiency,
 being two areas where work has been done.
 See 
\begin_inset CommandInset citation
LatexCommand cite
key "approx_bayes_comp"

\end_inset

 for an overview of areas of research.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Show MCMC method, and how SMC can be an alternative?
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Sequential Monte Carlo in an ABC setting
\end_layout

\begin_layout Standard
Sequential Monte Carlo (SMC) methods provide an alternative to MCMC methods
 when generating samples from a distribution, and there exist several variations
 on how it is used within ABC, see for example 
\begin_inset CommandInset citation
LatexCommand cite
key "Peters2012,Toni187,adaptive_smc_for_abc"

\end_inset

.
 An SMC method decomposes the problem of generating a sample from the target
 distribution into a sequence of steps that are smaller and simpler than
 trying to generate a sample directly.
 It does this by first generating a number of samples from an approximation
 to the target distribution, termed particles, and then updating these by
 guiding them through the series of further approximations to the target
 distribution.
 The output in the end is a set of weighted samples, which in turn can be
 used to approximate a sample set from the target distribution.
 The premise will be explained in detail, and we start with the concept
 of a weighted sample, which we illustrate by looking at the sampling importance
 resampling (SIR) method for generating samples from a distribution.
\end_layout

\begin_layout Subsection*
Sampling importance resampling algorithm
\end_layout

\begin_layout Standard
Suppose we have a 
\begin_inset Formula $k$
\end_inset

-dimensional random variable 
\begin_inset Formula $\boldsymbol{X}$
\end_inset

 coming from a distribution with probability density function 
\begin_inset Formula $p(\boldsymbol{x})$
\end_inset

, and that we are unable to generate samples it from because it is computational
ly intractable.
 Suppose further that we have another density defined on the same space
 that we are able to generate samples from, and that this density has probabilit
y density function 
\begin_inset Formula $g(\boldsymbol{x})$
\end_inset

 and the following holds:
\begin_inset Note Note
status open

\begin_layout Plain Layout
Show figure
\end_layout

\end_inset


\begin_inset Formula 
\[
g(\boldsymbol{x})\geq p(\boldsymbol{x}),\forall\boldsymbol{x}
\]

\end_inset


\end_layout

\begin_layout Standard
We can use this relationship to give the samples we draw from 
\begin_inset Formula $g(\cdot)$
\end_inset

 a weight based on how probable this sample is under 
\begin_inset Formula $p(\cdot)$
\end_inset

, and then use these samples and weights to generate samples that approximate
 the distribution of 
\begin_inset Formula $\boldsymbol{X}$
\end_inset

.
 Since we are drawing samples from 
\begin_inset Formula $g(\cdot)$
\end_inset

 we call it the proposal distribution.
 
\end_layout

\begin_layout Standard
To see how this relationship lets us approximate the distribution of 
\begin_inset Formula $\boldsymbol{X}$
\end_inset

, we start by defining the quantity known as the weight as 
\begin_inset Formula $w(\boldsymbol{x})=\frac{p(\boldsymbol{x})}{g(\boldsymbol{x})}$
\end_inset

, and 
\begin_inset Formula $\boldsymbol{Y}_{1},...,\boldsymbol{Y}_{n}$
\end_inset

 being draws from the distribution of 
\begin_inset Formula $g(\boldsymbol{x})$
\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Use better notation
\end_layout

\end_inset

.
\end_layout

\begin_layout Enumerate
Sample 
\begin_inset Formula $\boldsymbol{Y}_{1},...,\boldsymbol{Y}_{n}$
\end_inset

 from 
\begin_inset Formula $g(\boldsymbol{x})$
\end_inset

 
\end_layout

\begin_layout Enumerate
Calculate the weights, 
\begin_inset Formula $w(\boldsymbol{Y}_{i})=\frac{p(\boldsymbol{Y}_{i})}{g(\boldsymbol{Y}_{i})}$
\end_inset


\end_layout

\begin_layout Enumerate
Resample from the set 
\begin_inset Formula $\boldsymbol{Y}_{1},...,\boldsymbol{Y}_{n}$
\end_inset

 
\begin_inset Formula $m$
\end_inset

 times with probabilities for drawing 
\begin_inset Formula $\boldsymbol{Y}_{i}$
\end_inset

 being propotional to the weight 
\begin_inset Formula $w(\boldsymbol{Y}_{i})$
\end_inset

.
 Denote this resampled set as 
\begin_inset Formula $\boldsymbol{X}_{1},...,\boldsymbol{X}_{m}$
\end_inset

.
\end_layout

\begin_layout Standard
We now claim that the sample set 
\begin_inset Formula $\boldsymbol{X}_{1},...,\boldsymbol{X}_{m}$
\end_inset

 converges to the distribution for 
\begin_inset Formula $p(\boldsymbol{x})$
\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Use better notation
\end_layout

\end_inset

as 
\begin_inset Formula $m\to\infty$
\end_inset

.
 Following a similar route as found in 
\begin_inset CommandInset citation
LatexCommand cite
after "pp. 293-295"
key "ross2013simulation"

\end_inset

 we prove this by first noting that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P(\boldsymbol{X}\in A|\boldsymbol{Y}_{1},...,\boldsymbol{Y}_{n}) & =\frac{\sum_{i=1}^{n}I(\boldsymbol{Y}_{i}\in A)w(\boldsymbol{Y}_{i})}{\sum_{i=1}^{n}w(\boldsymbol{Y}_{i})}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
By the strong law of large numbers we get:
\begin_inset Note Note
status open

\begin_layout Plain Layout
Check strong law of large numbers
\end_layout

\end_inset


\begin_inset Formula 
\begin{align*}
\lim_{n\to\infty}\sum_{i=1}^{n}\frac{1}{n} & I(\boldsymbol{Y}_{i}\in A)w(\boldsymbol{Y}_{i})=\\
 & =E(I(\boldsymbol{Y}\in A)w(\boldsymbol{Y}))\\
 & =E(I(\boldsymbol{Y}\in A)w(\boldsymbol{Y})|I(\boldsymbol{Y}\in A))P(I(\boldsymbol{Y}\in A)\\
 & =E(w(\boldsymbol{Y})|\boldsymbol{Y}\in A)P(\boldsymbol{Y}\in A)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We also have:
\begin_inset Formula 
\begin{align*}
\lim_{n\to\infty}\sum_{i=1}^{n}\frac{1}{n}w(\boldsymbol{Y}_{i}) & =E(w(\boldsymbol{Y}))\\
 & =E(\frac{p(\boldsymbol{Y})}{g(\boldsymbol{Y})})\\
 & =\int\frac{p(\boldsymbol{y})}{g(\boldsymbol{y})}g(\boldsymbol{y})d\boldsymbol{y}\\
 & =\int p(\boldsymbol{y})d\boldsymbol{y}\\
 & =C
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $C$
\end_inset

 is an unknown constant.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{1}
\lim_{n\to\infty}P(\boldsymbol{X}\in A|\boldsymbol{Y}_{1},...,\boldsymbol{Y}_{n}) & =C^{-1}E(w(\boldsymbol{Y})|\boldsymbol{Y}\in A)P(\boldsymbol{Y}\in A)\\
 & =C^{-1}\int_{\boldsymbol{y}\in A}\frac{p(\boldsymbol{y})}{g(\boldsymbol{y})}g(\boldsymbol{y})d\boldsymbol{y}\\
 & =C^{-1}\int_{\boldsymbol{y}\in A}p(\boldsymbol{y})d\boldsymbol{y}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
On the other hand we have by Lebesgue's dominated convergence theorem:
\begin_inset Formula 
\[
P(\boldsymbol{X}\in A)=CE(P(\boldsymbol{X}\in A)|\boldsymbol{Y}_{1},...\boldsymbol{Y}_{n})=\int_{\boldsymbol{y}\in A}p(\boldsymbol{y})d\boldsymbol{y}
\]

\end_inset


\end_layout

\begin_layout Standard
The constant 
\begin_inset Formula $C$
\end_inset

 does not affect the sampling distribution, and we see that for large 
\begin_inset Formula $m$
\end_inset

 we get a good approximation to the distribution.
 In the next next we will extend the weight calculation step by introducing
 a recursive method.
\end_layout

\begin_layout Subsection*
Sequential Importance Sampling
\end_layout

\begin_layout Standard
The expression for the weight in the previous section, 
\begin_inset Formula $w(\boldsymbol{x})=\frac{p(\boldsymbol{x})}{g(\boldsymbol{x})}$
\end_inset

, can be rewritten as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w(\boldsymbol{x})=\frac{p(x_{1})p(x_{2}|x_{1})p(x_{3}|x_{1},x_{2})...p(x_{k}|x_{1},...,x_{k-1})}{g(x_{1})g(x_{2}|x_{1})g(x_{3}|x_{1},x_{2})...g(x_{k}|x_{1},...,x_{k-1})}
\]

\end_inset


\end_layout

\begin_layout Standard
If we define 
\begin_inset Formula $\boldsymbol{x}_{t}=(x_{1},...,x_{t})$
\end_inset

, the above can be written recursively as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w_{t}(\boldsymbol{x})=w_{t-1}(\boldsymbol{x}_{t-1})\frac{p(x_{t}|\boldsymbol{x}_{t-1})}{g(x_{t}|\boldsymbol{x}_{t-1})}
\]

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $1\leq t\leq k$
\end_inset

, we define 
\begin_inset Formula $w(\boldsymbol{x})=w_{k}(\boldsymbol{x}_{t})$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Check that the expression is correct
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Doing such a decomposition has some advantages when dealing with high-dimensiona
l problems, where it is difficult to find a good proposal distribution
\begin_inset CommandInset citation
LatexCommand cite
after "pp. 46-47"
key "liu2001monte"

\end_inset

.
 For one thing it lets us stop calculation of the weight early if we see
 it becomes too close to 0.
 We can also make use of 
\begin_inset Formula $p(x_{t}|\boldsymbol{x}_{t-1})$
\end_inset

 when setting up the proposal distribution 
\begin_inset Formula $g(x_{t}|\boldsymbol{x}_{t-1})$
\end_inset

.
\end_layout

\begin_layout Standard
There is a problem in that we need the marginal distributions, 
\begin_inset Formula $p(x_{1})$
\end_inset

,
\begin_inset Formula $p(x_{1},x_{2})$
\end_inset

,..., to get expressions for the conditional distributions.
 To get the marginal distributions there is a need to integrate out variables.
 For instance:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(x_{1},x_{2})=\int...\int p(x_{1},x_{2},...,x_{k})\mathrm{d}x_{3}...\mathrm{d}x_{k}
\]

\end_inset


\end_layout

\begin_layout Standard
This may be difficult, or impossible, and to get around this problem, we
 introduce another layer of complexity and look for a sequence of marginal
 distributions that are approximations to 
\begin_inset Formula $p(\boldsymbol{x}_{t})$
\end_inset

: 
\begin_inset Formula $\hat{p}(x_{1})$
\end_inset

, 
\begin_inset Formula $\hat{p}(x_{1},x_{2})$
\end_inset

,...,
\begin_inset Formula $p(x_{1},x_{2},...,x_{k})$
\end_inset

.
 The full joint distribution in the previous is not an approximation, but
 the actual 
\begin_inset Formula $p(\boldsymbol{x})$
\end_inset

.
 So we can view the approximations as converging to the true distribution.
 
\end_layout

\begin_layout Standard
In summary we are faced with two challenges: Finding good proposal distributions
, and approximations 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\hat{p}(x_{t}|\boldsymbol{x}_{t-1})$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add section on problems: Weight degeneracy.
 How to remedy: Resampling
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
SMC as part of an ABC setup
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Transition to how this is implemented in the code example
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
SMC in ABC
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}
\end_layout

\begin_layout Plain Layout


\backslash
Require $
\backslash
epsilon_{stop},
\backslash
beta,N$
\end_layout

\begin_layout Plain Layout


\backslash
State $
\backslash
epsilon 
\backslash
leftarrow 
\backslash
infty$
\end_layout

\begin_layout Plain Layout


\backslash
State $ess 
\backslash
leftarrow N$
\end_layout

\begin_layout Plain Layout


\backslash
State $
\backslash
boldsymbol{
\backslash
theta}_{n} 
\backslash
leftarrow$ $N$ samples from $p(
\backslash
boldsymbol{
\backslash
theta})$
\end_layout

\begin_layout Plain Layout


\backslash
State $particles 
\backslash
leftarrow $ $N$ samples from $p(
\backslash
boldsymbol{x}|
\backslash
boldsymbol{
\backslash
theta})$
\end_layout

\begin_layout Plain Layout


\backslash
State $
\backslash
boldsymbol{W}_{n}
\backslash
leftarrow [
\backslash
frac{1}{N},...,
\backslash
frac{1}{N}]$
\end_layout

\begin_layout Plain Layout


\backslash
While{$
\backslash
epsilon 
\backslash
geq 
\backslash
epsilon_{stop}$}
\end_layout

\begin_layout Plain Layout


\backslash
State $
\backslash
epsilon 
\backslash
leftarrow FindNextEpsilon(
\backslash
epsilon,
\backslash
boldsymbol{W}_{n})$
\end_layout

\begin_layout Plain Layout


\backslash
If{$ess < 
\backslash
beta N$}
\end_layout

\begin_layout Plain Layout


\backslash
State $(
\backslash
boldsymbol{
\backslash
theta}, particles) 
\backslash
leftarrow 
\backslash
mathrm{Resample}$
\end_layout

\begin_layout Plain Layout


\backslash
EndIf
\end_layout

\begin_layout Plain Layout


\backslash
For{$i
\backslash
leftarrow[1,..,N]$}
\end_layout

\begin_layout Plain Layout


\backslash
State $(
\backslash
boldsymbol{
\backslash
theta}_{n}^{(i)}, particles_{n}^{(i)})
\backslash
leftarrow 
\backslash
mathrm{Mutate}(
\backslash
epsilon, particles^{(i)}, W_{n-1}^{(i)}, 
\backslash
boldsymbol{
\backslash
theta}^{(i)})$
\end_layout

\begin_layout Plain Layout


\backslash
EndFor
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\begin_layout Plain Layout


\backslash
newline
\end_layout

\begin_layout Plain Layout


\backslash
Function{ESS}{$
\backslash
boldsymbol{W_{n}}$}
\end_layout

\begin_layout Plain Layout


\backslash
State 
\backslash
textbf{return} $(
\backslash
sum_{i=1}^{N}W_{n}^{(i)})^{-1}$
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
newline
\end_layout

\begin_layout Plain Layout


\backslash
Function{FindNextEpsilon}{$
\backslash
epsilon_{n-1},
\backslash
boldsymbol{W_{n-1}}$} 
\end_layout

\begin_layout Plain Layout


\backslash
State Solve for $
\backslash
epsilon_{n}$ $ESS(
\backslash
frac{I_{
\backslash
epsilon_{n}}(
\backslash
boldsymbol{x}_{k})}{I_{
\backslash
epsilon_{n-1}}(
\backslash
boldsymbol{x}_{k})})=
\backslash
alpha ESS(
\backslash
boldsymbol{W_{n-1}})$
\end_layout

\begin_layout Plain Layout


\backslash
State 
\backslash
textbf{return} $
\backslash
epsilon_{n}$
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
newline
\end_layout

\begin_layout Plain Layout


\backslash
Function{Mutate}{$
\backslash
epsilon,particle,
\backslash
boldsymbol{W},
\backslash
boldsymbol{
\backslash
theta}_{old}$}
\end_layout

\begin_layout Plain Layout


\backslash
State $
\backslash
boldsymbol{
\backslash
theta}_{new} 
\backslash
leftarrow$ Sample from $q(
\backslash
boldsymbol{
\backslash
theta}|
\backslash
boldsymbol{
\backslash
theta}_{old})$
\end_layout

\begin_layout Plain Layout


\backslash
State $particle_{new} 
\backslash
leftarrow$ Sample from $p(
\backslash
boldsymbol{x}|
\backslash
boldsymbol{
\backslash
theta}_{new})$
\end_layout

\begin_layout Plain Layout


\backslash
State $r 
\backslash
leftarrow 
\backslash
min(1,
\backslash
frac{I_{
\backslash
epsilon_{n}}(particle_{new})}{I_{
\backslash
epsilon_{n}}(particle)})$
\end_layout

\begin_layout Plain Layout


\backslash
If{$r < $ Sample from $Uniform(0, 1)$}
\end_layout

\begin_layout Plain Layout


\backslash
State 
\backslash
textbf{return} $(
\backslash
boldsymbol{
\backslash
theta},particle)$
\end_layout

\begin_layout Plain Layout


\backslash
Else
\end_layout

\begin_layout Plain Layout


\backslash
State 
\backslash
textbf{return} $(
\backslash
boldsymbol{
\backslash
theta}_{new},particle_{new})$
\end_layout

\begin_layout Plain Layout


\backslash
EndIf
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
newline
\end_layout

\begin_layout Plain Layout


\backslash
Function{Resample}{$
\backslash
boldsymbol{
\backslash
theta}, particles, 
\backslash
boldsymbol{W}$}
\end_layout

\begin_layout Plain Layout


\backslash
State Resample
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Assuming that 
\begin_inset Formula $d_{\epsilon}(\boldsymbol{x},\boldsymbol{y})=0$
\end_inset

 implies that 
\begin_inset Formula $\boldsymbol{x}=\boldsymbol{y}$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Make more clear
\end_layout

\end_inset

, we can set up a sequence of approxmations to the target distribution (which
 is the posterior distribution 
\begin_inset Formula $p(\boldsymbol{\theta}|\boldsymbol{y})$
\end_inset

 in our case):
\begin_inset Formula 
\[
p_{\epsilon_{1}}(\boldsymbol{\theta},\boldsymbol{x}|\boldsymbol{y}),p_{\epsilon_{2}}(\boldsymbol{\theta},\boldsymbol{x}|\boldsymbol{y}),...,p_{\epsilon_{n}}(\boldsymbol{\theta},\boldsymbol{x}|\boldsymbol{y})
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Setup definition
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\epsilon_{1}>\epsilon_{2}>...>\epsilon_{n}$
\end_inset

.
 If 
\begin_inset Formula $\epsilon_{n}=0$
\end_inset

 the distribution is equal to the actual posterior.
\end_layout

\begin_layout Standard
Using this series approximations to the posterior, we are left with the
 task of defining proposal distributions.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Focus on several methods?
\end_layout

\end_inset

We will focus on a method described in 
\begin_inset CommandInset citation
LatexCommand cite
key "adaptive_smc_for_abc"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Section on kernels.
 Update schedules
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "references"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
